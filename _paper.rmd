---
title: "Methods for understanding the variable importance of local explanations of black-box models"
# author:
#   - name: Nicholas Spyrison
#     address: |
#       | Monash University
#       | Faculty of Information Technology
#       | ORCiD: 0000-0002-8417-0212
#     email: \email{nicholas.spyrison@monash.edu}
#     orcid: 0000-0002-8417-0212
#   - name: Dianne Cook
#     affiliation: Monash University
#     address: |
#       | ORCiD: 000-0002-3813-7155
#     orcid: 000-0002-3813-7155
#   - name: Kimbal Marriott
#     affiliation: Monash University
#     address: |
#       | ORCiD: 0000-0002-9813-0377
#     orcid: 0000-0002-9813-0377
# keywords:
#   formatted: [XAI, explanable artificial intelligence, black-box models, local explanations, grand tour, manual tour, DR, dimension reduction, R]
#   plain:     [XAI, explanable artificial intelligence, black-box models, local explanations, grand tour, manual tour, DR, dimension reduction, R]
bibliography: cheem_paper.bib
abstract: |
  Artificial Intelligence (AI) has seen a revitalization in recent years from the use of increasingly hard-to-interpret black-box models. In such models, increased predictive power comes at the cost of opaque factor analysis, which has led to the field of explainable AI (XAI). XAI attempts to shed light on these models, one such approach is the use of local explanations. A local explanation of a model give a point-estimate of linear variable importance in the vicinity of one observation. We extract explanations for each observation, and approximate data and this attribution space side-by-side with linked brushing. After identifying an observation of interest its local explanation is used as a 1D projection basis. We then manipulate the magnitude of the variable contributions with a technique called the tour. This tour animates many projections over small changes in the projection basis. Doing so allows a user to visually explore the data space through the lens of this local explanation and interrogate its variable importance. The implementation of our framework is available as an R package called cheem available at [github.com/nspyrison/cheem](https://github.com/nspyrison/cheem).
# documentclass: jdssv
# classoption: article
output:
  bookdown::pdf_document2:
    keep_tex: true
    toc: false
    #template: template/jdssv_template.tex
    number_sections: yes
    fig_caption: yes
appendix:
  # - "appendix-1.Rmd"
# title:
#   formatted: "Methods for understanding the variable importance of local explanations of black-box models"
#   plain:     "Methods for understanding the variable importance of local explanations of black-box models"
#   short:     "Use of the manual tour to visually interrogate local explanations"
editor_options:
  chunk_output_type: console
urlcolor: blue
linkcolor: red
header-includes:
 - \usepackage{setspace} ### for title page spacing
 - \usepackage{hyperref} ### for all sorts of linking
 - \usepackage{graphicx} ### to insert Monash crest
---
```{r include=FALSE, cache=FALSE}
require("knitr")
require("kableExtra")
require("magrittr")
## Work packages
require("cheem")
require("spinifex")
## chunk options
knitr::opts_chunk$set(
  fig.align = "center",
  echo = FALSE, ## Code
  collapse = TRUE,
  message = FALSE,
  warning = FALSE,
  error = FALSE,
  cache = TRUE,
  cache.lazy = FALSE)
```

<!-- # cheat sheet {#sec:cheatsheet} -->
<!-- A bib reference [@wickham_visualizing_2015]. -->
<!-- A [Section intro](#sec:cheatsheet) reference, alternatively, section ef{sec:intro} (with no @; \\ref{sec:intro}). -->
<!-- ```{r crest, echo=FALSE, out.height = "10%", out.width = "10%", fig.cap = "A caption for crest figure"} -->
<!-- knitr::include_graphics("./figures/crest.jpg") -->
<!-- ``` -->
<!-- A figure \@ref(fig:crest) reference (with @; \\@ref(fig:crest). -->
<!-- ref:myFig-cap) Separate caption created above the R chunk -->
<!-- ```{r step2, echo=F, fig.cap = "(ref:myFig-cap)"} -->
<!-- knitr::include_graphics("./figures/crest.jpg") -->
<!-- ``` -->

<!-- GENERAL STRUCTURE:: -->
<!-- -- couple pages of general intro -->
<!-- -- tie into XAI -->
<!-- -- explain tour & manual tour -->
<!-- -- example: details on SHAP values -->
<!-- -- what you could learn about from classification and what from regression -->
<!-- -- section on software/interface: -->


# Introduction {#sec:intro}
<!-- WHAT TOPICS, and ISSUES addressed (motivation) -->
## MODELING
## XAI & interpretability crisis
## Local explanations
## Data visualization tours

# SHAP local explanation
## Variable importance, permuting over the X's included
## Visualizing and break down plots

# Interrogate variable imporances of the local explanations: Cheem
## RF model
## shap matrix
## Linked global approximations of data and local attribution spaces
## Applcation of the manual tour to intterogate local explanations
## Classification task
## Regression task

# Application Design
## 1) Penguins speicies classification
## 2) FIFA wage regression
## 3)?
## 4)?

# Software Infrastructure
## Extend spinifex, consume DALEX & treeshap
## Preprocess
## Runtime rendering

# Discsussion

# Acknoledgements

# References
