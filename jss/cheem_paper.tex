% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdfkeywords={true},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{true}
\author{true \and true \and true}
\date{}

\begin{document}
\maketitle
\begin{abstract}
The increased predictive power of machine learning models comes at the cost of increased complexity and loss of interpretability, particularly in comparison to parametric statistical models. This trade-off has led to the emergence of eXplainable AI (XAI) which provides methods, such as local explanations (LEs) and local variable attributions (LVAs), to shed light on how a model use predictors to arrive at a prediction. These provide a point estimate of the linear variable importance in the vicinity of a single observation. However, LVAs tend not to effectively handle association between predictors. To understand how the interaction between predictors affects the variable importance estimate, we can convert LVAs into linear projections and use the radial tour. This is also useful for learning how a model has made a mistake, or the effect of outliers, or the clustering of observations. The approach is illustrated with examples from categorical (penguin species, chocolate types) and quantitative (soccer/football salaries, house prices) response models. The methods are implemented in the R package cheem, available on CRAN.
\end{abstract}

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\hypertarget{sec:intro}{%
\section{Introduction}\label{sec:intro}}

There are different reasons and purposes for fitting a model. According to the taxonomies of Breiman (2001b) and Shmueli (2010), it can be useful to group models into two types: explanatory and predictive. Explanatory modeling is used for inferential purposes, while predictive modeling focuses solely on the performance of an objective function. The intended use of the model has important implications for its selection and development. Interpretability is critical in explanatory modeling to draw meaningful inferential conclusions, such as which variables most contribute to a prediction or whether some observations are less well fit. Interpretability becomes more difficult when the model is nonlinear. Nonlinear models occur in statistical models with polynomial or interaction terms between quantitative predictors, and almost all computational models such as random forests, support vector machines, or neural networks (e.g. Breiman 2001a; Boser, Guyon, and Vapnik 1992; Anderson 1995).

In linear models interpretation of the importance of variables is relatively straightforward, one adjusts for the covariance of multiple variables when examining the relationship with the response. The interpretation is valid for the full domain of the predictors. In nonlinear models, one needs to consider the model in small neighborhoods of the domain to make any assessment of variable importance. Even though this is difficult, it is especially important to interpret model fits as we become more dependent on nonlinear models for routine aspects of life to avoid issues described in Stahl (2021). Understanding how nonlinear models behave when usage extrapolates outside the domain of predictors, either in sub-spaces where few samples were provided in the training set, or extending outside the domain. It is especially important because nonlinear models can vary wildly and predictions can be dramatically wrong in these areas.

Explainable Artificial Intelligence (XAI) is an emerging field of research focused on methods for the interpreting of models (Adadi and Berrada 2018; Barredo Arrieta et al. 2020). A class of techniques, called \emph{local explanations} (LEs), provide methods to approximate linear variable importance, called local variable attributions (LVAs), at the location of each observation or the predictions at a specific point in the data domain. Because these are point-specific, it is challenging to comprehensively visualize them to understand a model. There are common approaches for visualizing high-dimensional data as a whole, but what is needed are new approaches for viewing these individual LVAs relative to the whole.

For multivariate data visualization, a \emph{tour} (Asimov 1985; Buja and Asimov 1986; S. Lee et al. 2021) of linear data projections onto a lower-dimensional space, could be an element of XAI, complementing LVAs.
Applying tours to model interpretation is recommended by Wickham, Cook, and Hofmann (2015) primarily to examine the fitted model in the space of the data. Cook, Swayne, and Buja (2007) describe the use of tours for exploring classification boundaries and model diagnostics (Caragea et al. 2008; Y. D. Lee et al. 2013; da Silva, Cook, and Lee 2021).
There are various types of tours. In a \emph{manual} or radial tour (Cook and Buja 1997; Spyrison and Cook 2020), the path of linear projections is defined by changing the contribution of a selected variable. We propose to use this to scrutinize the LVAs.
This approach could be considered to be a counter-factual, what-if analysis, such as \emph{ceteris paribus} (``other things held constant'') profiles (Biecek 2020).

The remainder of this paper is organized as follows. Section \ref{sec:explanations} covers the background of the LEs and the traditional visuals produced. Section \ref{sec:tour} explains the tours and particularly the radial manual tour. Section \ref{sec:cheemviewer} discusses the visual layout in the graphical user interface and how it facilitates analysis, data pre-processing, and package infrastructure. Illustrations are provided in Section \ref{sec:casestudies} for a range of supervised learning tasks with categorical and quantitative response variables. These show how the LVAs can be used to get an overview of the model's use of predictors and to investigate errors in the model predictions. Section \ref{sec:cheemdiscussion} concludes with a summary of the insights gained. The methods are implemented in the \textbf{R} package \textbf{cheem}.

\hypertarget{sec:explanations}{%
\section{Local Explanations}\label{sec:explanations}}

LVAs shed light on machine learning model fits by estimating linear variable importance in the vicinity of a single observation. There are many approaches for calculating LVAs.
A comprehensive summary of the taxonomy of currently available methods is provided in Figure 6 by Barredo Arrieta et al. (2020). It includes a large number of model-specific explanations such as deepLIFT (Shrikumar et al. 2016; Shrikumar, Greenside, and Kundaje 2017), a popular recursive method for estimating importance in neural networks. There are fewer model-agnostic methods, of which LIME (Ribeiro, Singh, and Guestrin 2016) and SHaply Additive exPlanations (SHAP) (Lundberg and Lee 2017), are popular.

These observation-level explanations are used in various ways depending on the data. In image classification, where pixels correspond to predictors, saliency maps overlay or offset a heatmap to indicate important pixels (Simonyan, Vedaldi, and Zisserman 2014). For example, pixels corresponding to snow may be highlighted as important contributors when distinguishing if a picture contains a coyote or husky. In text analysis, word-level contextual sentiment analysis highlights the sentiment and magnitude of influential words (Vanni et al. 2018). In the case of numeric regression, they are used to explain additive contributions of variables from the model intercept to the observation's prediction (Ribeiro, Singh, and Guestrin 2016).

We will be focusing on SHAP values in this paper, but the approach is applicable to any method used to calculate the LVAs. SHAP calculates the variable contributions of one observation by examining the effect of other variables on the predictions. The term ``SHAP'' refers to Shapley (1953)'s method to evaluate an individual's contribution in cooperative games by assessing this player's performance in the presence or absence of other players. Strumbelj and Kononenko (2010) introduced SHAP for LEs in machine learning models. Variable importance can depend on the sequence in which variables are entered into the model fitting process, thus for any sequence we get a set of variable contribution values for a single observation. These values will add up to the difference between the fitted value for the observation, and the average fitted value for all observations. Using all possible sequences, or permutations, gives multiple values for each variable, which are averaged to get the SHAP value for an observation. It can be helpful to standardize variables prior to computing SHAP values if they have been measured on different scales.

The approach is related to partial dependence plots (for example see chapter 8 of Molnar (2022)), used to explain the effect of a variable by predicting the response for a range of values on this variable after fixing the value of all other variables to their mean. Though partial dependence plots are a global approximation of the variable importance, while SHAP is specific to one observation.

\begin{figure}

{\centering \includegraphics[width=0.85\linewidth]{./figures/shap_distr_bd} 

}

\caption{Illustration of SHAP values for a random forest model FIFA 2020 player wages from nine skill predictors. A star offensive and defensive player are compared, L. Messi and V. van Dijk, respectively. Panel (a) shows breakdown plots of three sequences of the variables. The sequence of the variables impacts the magnitude of their attribution. Panel (b) shows the distribution of attribution for each variable across 25 sequences of predictors, with the mean displayed as a dot for each player. Reaction skills are important for both players. Offense and movement are important for Messi but not van Dijk, and conversely, defense and power are important for van Dijk but not Messi.}\label{fig:shapdistrbd}
\end{figure}

We use 2020 season FIFA data (Leone 2020) to illustrate SHAP following the procedures described in Biecek and Burzykowski (2021). There are 5000 observations of nine predictor variables measuring players' skills and one response variable, wages (in euros). A random forest model is fit regressing players' wages on the skill variables. In this illustration in Figure \ref{fig:shapdistrbd} the SHAP values are compared for a star offensive player (L. Messi) and a prominent defensive player (V. van Dijk). We are interested in knowing how the skill variables locally contribute to the wage prediction of each player. A difference in the attribution of the variable importance across the two positions of the players can be expected. This would be interpreted as how a player's salary depends on which combination of skills. Panel (a) is a version of a breakdown plot (Gosiewska and Biecek 2019) where just three sequences of variables are shown, for two observations. A breakdown plot shows the absolute values of the variable attribution for an observation, usually sorted from the highest value to the lowest. There is no scale on the horizontal axis here because values are considered relative to each other. Here we can see how the variable contribution can change depending on sequence, relative to both players. (Note that the order of the variables is different in each plot because they have been sorted by the biggest average contribution across both players.) For all sequences, and for both players \texttt{reaction} has the strongest contribution, with perhaps more importance for the defensive player. Then it differs by player: for Messi \texttt{offense} and \texttt{movement} have the strongest contributions, and for van Dijk it is \texttt{defense} and \texttt{power}, regardless of the variable sequence.

Panel (b) shows the differences in the player's median values (large dots) for 25 such sequences (tick marks). We can see that the wage predictions for the two players come from different combinations of skill sets, as might be expected for players whose value on the team depends on their offensive or defensive prowess. It is also interesting to see from the distribution of values across the different sequences of variables, that there is some multimodality. For example, look at the SHAP values for \texttt{reaction} for Messi, and in some sequences, reaction has a much lower contribution than others. This suggests that other variables (\texttt{offense}, \texttt{movement} probably) can substitute for \texttt{reaction} in the wage prediction.

This can also be considered similar to examining the coefficients from all subsets regression, as described in Wickham, Cook, and Hofmann (2015). Various models that are similarly good might use different combinations of the variables. Examining the coefficients from multiple models helps to understand the relative importance of each variable in the context of all other variables. This is similar to the approach here with SHAP values, that by examining the variation in values across different permutations of variables, we can gain more understanding of the relationship between the response and predictors.

For the application, we use \emph{tree SHAP}, a variant of SHAP that enjoys a lower computational complexity (Lundberg, Erion, and Lee 2018). Instead of aggregating over sequences of the variables, tree SHAP calculates observation-level variable importance by exploring the structure of the decision trees. Tree SHAP is only compatible with tree-based models. so random forests are used for illustration.

There are numerous R packages currently available that provide functions for computing SHAP values, including \texttt{fastshap} (Greenwell 2021), \texttt{kernelshap} (Mayer and Watson 2023), \texttt{shapr} (Sellereite, Jullum, and Redelmeier 2023), \texttt{shapviz} (Mayer 2023b), \texttt{PPtreeregViz} (E.-K. Lee and Cho 2022), \texttt{ExplainPrediction} (Robnik-Sikonja 2018), \texttt{flashlight} (Mayer 2023a), and the package \texttt{DALEX} has many resources (Biecek 2018). There are many more packages only available through Github, like \texttt{treeshap} (Kominsarczyk et al. 2021) that is used for this work. Molnar (2022) provides good explanations of the different methods and how to apply them to different models.

\hypertarget{sec:tour}{%
\section{Tours and the Radial Tour}\label{sec:tour}}

A \emph{tour} enables the viewing of high-dimensional data by animating many linear projections with small incremental changes. It is achieved by following a path of linear projections (bases) of high-dimensional space. One key variable of the tour is the object permanence of the data points; one can track the relative change of observations in time and gain information about the relationships between points across multiple variables. There are various types of tours that are distinguished by how the paths are generated (S. Lee et al. 2021; Cook et al. 2008).

The manual tour (Cook and Buja 1997) defines its path by changing a selected variable's contribution to a basis to allow the variable to contribute more or less to the projection. The requirement constrains the contribution of all other variables that a basis needs to be orthonormal (columns correspond to vectors, with unit length, and orthogonal to each other). The manual tour is primarily used to assess the importance of a variable to the structure visible in a projection. It also lends itself to pre-computation queued in advance or computed on the fly for human-in-the-loop analysis (Karwowski 2006).

A version of the manual tour called a \emph{radial tour} is implemented in Spyrison and Cook (2020) and forms the basis of this new work. In a radial tour, the selected variable can change its magnitude of contribution but not its angle; it must move along the direction of its original contribution. The implementation allows for pre-computation and interactive re-calculation to focus on a different variable.

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{./figures/radial_tour} 

}

\caption{The radial tour allows the user to remove a variable from a projection, to examine the importance of this variable to the structure in the plot. Here we have a 1D projection of the penguins data displayed as a density plot. The line segments on the bottom correspond to the coefficients of the variables making up the projection. The structure in the plot is bimodality (left), and the importance of the variable \textsf{bd} is being explored. As this variable contribution is reduced in the plot (middle, right) we can see that the bimodality decreases. Thus \textsf{bd} is an important variable contributing to the bimodal structure.}\label{fig:radialtour}
\end{figure}

\hypertarget{sec:cheemviewer}{%
\section{The Cheem Viewer}\label{sec:cheemviewer}}

To explore the LVAs, coordinated views (Roberts 2007) (also known as ensemble graphics, Unwin and Valero-Mora 2018) are provided in the \emph{cheem viewer} application. There are two primary plots: the \textbf{global view} to give the context of all of the SHAP values and the \textbf{radial tour view} to explore the LVAs with user-controlled rotation. There are numerous user inputs, including variable selection for the radial tour and observation selection for making comparisons. There are different plots used for the categorical and quantitative responses. Figures \ref{fig:classificationcase} and \ref{fig:regressioncase} are screenshots showing the cheem viewer for the two primary tasks: classification (categorical response) and regression (quantitative response).

\hypertarget{global-view}{%
\subsection{Global View}\label{global-view}}

The global view provides context for all observations and facilitates the exploration of the separability of the data and attribution spaces. The attribution space refers to the SHAP values for each observation. These spaces both have dimensionality \(n \times p\), where \(n\) is the number of observations and \(p\) is the number of variables.

The visualization is composed of the first two principal components of the data (left) and the attribution (middle) spaces. These single 2D projections will not reveal all of the structure of higher-dimensional space, but they are helpful visual summaries. In addition, a plot of the observed against predicted response values is also provided (Figures \ref{fig:classificationcase}b, \ref{fig:regressioncase}a) to help identify observations poorly predicted by the model. For classification tasks, color indicates the predicted class and misclassified observations are circled in red. Linked brushing between the plots is provided, and a tabular display of selected points helps to facilitate the exploration of the spaces and the model (shown in Figures \ref{fig:regressioncase}d).

While the comparison of these spaces is interesting, the primary purpose of the global view is to enable the selection of particular observations to explore in detail. We have designed it to enable a comparison between an observation that is interesting in some way, perhaps misclassified, or poorly predicted, relative to an observation with similar predictor values but a more expected prediction. For brevity, we call the interesting observation the primary investigation (PI), and the other is the comparison investigation (CI). These observations are highlighted as an asterisk and \(\times\), respectively.

\hypertarget{radial-tour}{%
\subsection{Radial Tour}\label{radial-tour}}

There are two plots in this part of the interface. The first (Figures \ref{fig:classificationcase}e and \ref{fig:regressioncase}e) is a display of the SHAP values for all observations. This will generally give the global view of variables important for the fit as a whole, but it will also highlight observations that have different patterns. The second plot is the radial tour, which for classification is a density plot of a 1D projection (Figure \ref{fig:classificationcase}f), and for regression are scatterplots of the observed response values, and residuals, against a 1D projection (Figure \ref{fig:regressioncase}f).

The LVAs for all observations are normalized (sum of squares equals 1), and thus, the relative importance of variables can be compared across all observations. These are depicted as a vertical parallel coordinate plot (Ocagne 1885). (The SHAP values of the PI and CI are shown as dashed and dotted lines, respectively.) One should obtain a sense of the overall importance of variables from this plot. The more important variables will have larger values, and in the case of classification tasks variables that have different magnitudes for different classes are more globally important. For example, Figure \ref{fig:classificationcase}e suggests that \texttt{bl} is important for distinguishing the green class from the other two. For regression, one might generally observe which variables have low values for all observations (not important). For example, \texttt{BMI} and \texttt{pwr} in Figure \ref{fig:regressioncase}e, have a range of high and low values (e.g., \texttt{off}, \texttt{def}), suggesting they are important for some observations and not important for others.

A bar chart is overlaid to represent the projection shown in the radial tour on the right. It starts from the SHAP values of the PI, but if the user changes the projection the length of these bars will reflect this change. (The PI is interactively selected by clicking on a point in the global view). By scaling the SHAP value it becomes an (attribution) projection.

The attribution projection of the PI is the initial 1D basis in a radial tour, displayed as a density plot for a categorical response (Figure \ref{fig:classificationcase}f) and as scatterplots for a quantitative response (Figure \ref{fig:regressioncase}f). The PI and CI are indicated by vertical dashed and dotted lines, respectively. The radial tour varies the contribution of the selected variable. This is viewed as an animation of the projections from many intermediate bases. Doing so tests the sensitivity of structure (class separation or strength of relationship) to the variable's contribution. For classification, if the separation between classes diminishes when the variable contribution is reduced, this suggests that the variable is important for class separation. For regression, if the relationship scatterplot weakens when the variable contribution is reduced, indicating that the variable is important for accurately predicting the response.

\hypertarget{classification-task}{%
\subsection{Classification Task}\label{classification-task}}

Selecting a misclassified observation as PI and a correctly classified point nearby in data space as CI makes it easier to examine the variables most responsible for the error. The global view (Figure \ref{fig:classificationcase}c) displays the model confusion matrix. The radial tour is 1D and displays as density where color indicates class. An animation slider enables users to vary the contribution of variables to explore the sensitivity of the separation to that variable.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{./figures/app_classification} 

}

\caption{Overview of the cheem viewer for classification tasks (categorical response). Global view inputs, (a), set the PI, CI, and color statistic. Global view, (b) PC1 by PC2 approximations of the data- and attribution-space. (c) prediction by observed $y$ (visual of the confusion matrix for classification tasks). Points are colored by predicted class, and red circles indicate misclassified observations. Radial tour inputs (d) select variables to include and which variable is changed in the tour. (e) shows a parallel coordinate display of the distribution of the variable attributions while bars depict contribution for the current basis. The black bar is the variable being changed in the radial tour. Panel (f) is the resulting data projection indicated as density in the classification case.}\label{fig:classificationcase}
\end{figure}

\hypertarget{regression-task}{%
\subsection{Regression Task}\label{regression-task}}

Selecting an inaccurately predicted observation as PI and an accurately predicted observation with similar variable values as CI is a helpful way to understand how the model is failing or not. The global view (Figure \ref{fig:regressioncase}a) shows a scatterplot of the observed vs predicted values, which should exhibit a strong relationship if the model is a good fit. The points can be colored by a statistic, residual, a measure of outlyingness (log Mahalanobis distance), or correlation to aid in understanding the structure identified in these spaces.

In the radial tour view, the observed response and the residuals (vertical) are plotted against the attribution projection of the PI (horizontal). The attribution projection can be interpreted similarly to the predicted value from the global view plot. It represents a linear combination of the variables, and a good fit would be indicated when there is a strong relationship with the observed values. This can be viewed as a local linear approximation if the fitted model is nonlinear. As the contribution of a variable is varied, if the value of the PI does not change much, it would indicate that the prediction for this observation is NOT sensitive to that variable. Conversely, if the predicted value varies substantially, the prediction is very sensitive to that variable, suggesting that the variable is very important for the PI's prediction.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{./figures/app_regression_interactions} 

}

\caption{Overview of the cheem viewer for regression tasks (quantitative response) and illustration of interactive variables. Panel (a) PCA of the data- and attributions- spaces and the (b) residual plot, predictions by observed values. Four selected points are highlighted in the PC spaces and tabularly displayed. Coloring on a statistic (c) highlights the structure organized in the attribution space. Interactive tabular display (d) populates when observations are selected. Contribution of the 1D basis affecting the horizontal position (e) parallel coordinate display of the variable attribution from all observations, and horizontal bars show the contribution to the current basis. Regression projection (f) uses the same horizontal projection and fixes the vertical positions to the observed $y$ and residuals (middle and right).}\label{fig:regressioncase}
\end{figure}

\hypertarget{interactive-variables}{%
\subsection{Interactive variables}\label{interactive-variables}}

The application has several reactive inputs that affect the data used, aesthetic display, and tour manipulation. These reactive inputs make the software flexible and extensible (Figure \ref{fig:classificationcase}a \& d). The application also has more exploratory interactions to help link points across displays, reveal structures found in different spaces, and access the original data.

A tooltip displays the observation number/name and classification information while the cursor hovers over a point. Linked brushing allows the selection of points (left click and drag) where those points will be highlighted across plots (Figure \ref{fig:classificationcase}a \& b). The information corresponding to the selected points is populated on a dynamic table (Figure \ref{fig:classificationcase}d). These interactions aid the exploration of the spaces and, finally, the identification of primary and comparison observations.

\hypertarget{preprocessing}{%
\subsection{Preprocessing}\label{preprocessing}}

It is vital to mitigate the render time of visuals, especially when users may want to iterate many explorations. All computational operations should be prepared before run time. The work remaining when an application is run solely reacts to inputs and rendering visuals and tables. Below discusses the steps and details of the reprocessing.

\begin{itemize}
\tightlist
\item
  \textbf{Data:} predictors and response are unscaled complete numerical matrix. Most models and local explanations are scale-invariant. Keep the normality assumptions of the model in mind.
\item
  \textbf{Model:} any model and compatible explanation could be explored with this method. Currently, random forest models are applied via the package \textbf{randomForest} (Liaw and Wiener 2002), compatibility tree SHAP. Modest hyperparameters are used, namely: 125 trees, the number of variables at each split, mtry = \(\sqrt{p}\) or \(p/3\) for classification and regression, and minimum size of terminal nodes \(max(1, n/500)\) or \(max(5, n/500)\) for classification and regression.
\item
  \textbf{Local explanation:} Tree SHAP is calculated for \emph{each} observation using the package \textbf{treeshap} (Kominsarczyk et al. 2021). We opt to find the attribution of each observation in the training data and not fit to fit variable interactions.
\item
  \textbf{Cheem viewer:} after the model and full explanation space are calculated, each variable is scaled by standard deviations away from the mean to achieve common support for visuals. Statistics for mapping to color are computed on the scaled spaces.
\end{itemize}

The time to preprocess the data will vary significantly with the complexity of the model and the LE. For reference, the FIFA data contained 5000 observations of nine explanatory variables that took 2.5 seconds to fit a random forest model of modest hyperparameters. Extracting the tree SHAP values of each observation took 270 seconds in total. PCA and statistics of the variables and attributions took 2.8 seconds. These run times were from a non-parallelized session on a modern laptop, but suffice it to say that most of the time will be spent on the LVA. An increase in model complexity or data dimensionality will quickly become an obstacle. Its reduced computational complexity makes tree SHAP an excellent candidate to start. Alternatively, some package and methods use approximate calculations of LEs, such as \textbf{fastshap} Greenwell (2020).

\hypertarget{sec:casestudies}{%
\section{Case Studies}\label{sec:casestudies}}

To illustrate the cheem method it is applied to modern data sets, two classification examples and then two of regression.

\hypertarget{palmer-penguin-species-classification}{%
\subsection{Palmer Penguin, Species Classification}\label{palmer-penguin-species-classification}}

The Palmer penguins data (Gorman, Williams, and Fraser 2014; Horst, Hill, and Gorman 2020) was collected on three species of penguins foraging near Palmer Station, Antarctica. The data is publicly available to substitute for the overly-used iris data and is quite similar in form. After removing incomplete observations, there are 333 observations of four physical measurements, bill length (\texttt{bl}), bill depth (\texttt{bd}), flipper length (\texttt{fl}), and body mass (\texttt{bm}) for this illustration. A random forest model was fit with species as the response variable.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{./figures/case_penguins} 

}

\caption{Examining the SHAP values for a random forest model classifying Palmer penguin species. The PI is a Gentoo (purple) penguin that is misclassified as a Chinstrap (orange), marked as an asterisk in (a) and the dashed vertical line in (b). The radial view shows varying the contribution of `fl` from the initial attribution projection (b, left), which produces a linear combination where the PI is more probably (higher density value) a Chinstrap than a Gentoo (b, right). (The animation of the radial tour is at https://vimeo.com/666431172.)}\label{fig:casepenguins}
\end{figure}

Figure \ref{fig:casepenguins} shows plots from the cheem viewer for exploring the random forest model on the penguins data. Panel (a) shows the global view, and panel (b) shows several 1D projections generated with the radial tour. Penguin 243, a Gentoo (purple), is the PI because it has been misclassified as a Chinstrap (orange).

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{./figures/case_penguins_BlFl} 

}

\caption{Checking what is learned from the cheem viewer. This is a plot of flipper length (`fl`) and bill length (`bl`), where an asterisk highlights the PI. A Gentoo (purple) misclassified as a Chinstrap (orange). The PI has an unusually small `fl` length which is why it is confused with a Chinstrap.}\label{fig:casepenguinsblfl}
\end{figure}

There is more separation visible in the attribution space than in the data space, as would be expected. The predicted vs observed plot reveals a handful of misclassified observations. A Gentoo which has been wrongly labeled as a Chinstrap is selected for illustration. The PI is a misclassified point (represented by the asterisk in the global view and a dashed vertical line in the tour view). The CI is a correctly classified point (represented by an \(\times\) and a vertical dotted line).

The radial tour starts from the attribution projection of the misclassified observation (b, left). The important variables identified by SHAP in the (wrong) prediction for this observation are mostly \texttt{bl} and \texttt{bd} with small contributions of \texttt{fl} and \texttt{bm}. This projection is a view where the Gentoo (purple) looks much more likely for this observation than Chinstrap. That is, this combination of variables is not particularly useful because the PI looks very much like other Gentoo penguins. The radial tour is used to vary the contribution of flipper length (\texttt{fl}) to explore this. (In our exploration, this was the third variable explored. It is typically helpful to explore the variables with more significant contributions, here \texttt{bl} and \texttt{bd}. Still, when doing this, nothing was revealed about how the PI differed from other Gentoos). On varying \texttt{fl}, as it contributes increasingly to the projection (b, right), more and more, this penguin looks like a Chinstrap. This suggests that \texttt{fl} should be considered an important variable for explaining the (wrong) prediction.

Figure \ref{fig:casepenguinsblfl} confirms that flipper length (\texttt{fl}) is vital for the confusion of the PI as a Chinstrap. Here, flipper length and body length are plotted, and the PI can be seen to be closer to the Chinstrap group in these two variables, mainly because it has an unusually low value of flipper length relative to other Gentoos. From this view, it makes sense that it is a hard observation to account for, as decision trees can only partition only vertical and horizontal lines.

\hypertarget{chocolates-milkdark-classification}{%
\subsection{Chocolates, Milk/Dark Classification}\label{chocolates-milkdark-classification}}

The chocolates data set consists of 88 observations of ten nutritional measurements determined from their labels and labeled as either milk or dark. Dark chocolate is considered healthier than milk. Students collected the data during the Iowa State University class STAT503 from nutritional information on the manufacturer's websites and were normalized to 100g equivalents. The data is available in the \textbf{cheem} package. A random forest model is used for the classification of chocolate types.

It could be interesting to examine the nutritional properties of any dark chocolates that have been misclassified as milk. A reason to do this is that a dark chocolate, nutritionally more like milk should not be considered a healthy alternative. It is interesting to explore which nutritional variables contribute most to misclassification.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{./figures/case_chocolates} 

}

\caption{Examining the LVA for a PI which is dark (orange) chocolate incorrectly predicted to be milk (green). From the attribution projection, this chocolate correctly looks more like dark than milk, which suggests that the LVA does not help understand the prediction for this observation. So, the contribution of Sugar is varied---reducing it corresponds primarily with an increased magnitude from Fiber. When Sugar is zero, Fiber contributes strongly toward the left. In this view, the PI is closer to the bulk of the milk chocolates, suggesting that the prediction put a lot of importance on Fiber. This chocolate is a rare dark chocolate without any Fiber leading to it being mistaken for a milk chocolate. (A video of the tour animation can be found at https://vimeo.com/666431143.)}\label{fig:casechocolates}
\end{figure}

This type of exploration is shown in Figure \ref{fig:casechocolates}, where a chocolate labeled dark but predicted to be milk is chosen as the PI (observation 22). It is compared with a CI that is a correctly classified dark chocolate (observation 7). The PCA plot and the tree SHAP PCA plots (a) show a big difference between the two chocolate types but with confusion for a handful of observations. The misclassifications are more apparent in the observed vs predicted plot and can be seen to be mistaken in both ways: milk to dark and dark to milk.

The attribution projection for chocolate 22 suggests that Fiber, Sugars, and Calories are most responsible for its incorrect prediction. The way to read this plot is to see that Fiber has a large negative value while Sugars and Calories have reasonably large positive values. In the density plot, observations on the very left of the display would have high values of Fiber (matching the negative projection coefficient) and low values of Sugars and Calories. The opposite would be interpreting a point with high values in this plot. The dark chocolates (orange) are primarily on the left, and this is a reason why they are considered to be healthier: high fiber and low sugar. The density of milk chocolates is further to the right, indicating that they generally have low fiber and high sugar.

The PI (dashed line) can be viewed against the CI (dotted line). Now, one needs to pay attention to the parallel plot of the SHAP values, which are local to a particular observation, and the density plot, which is the same projection of all observations as specified by the SHAP values of the PI. The variable contribution of the two different predictions can be quickly compared in the parallel coordinate plot. The PI differs from the comparison primarily on the Fiber variable, which suggests that this is the reason for the incorrect prediction.

From the density plot, which is the attribution projection corresponding to the PI, both observations are more like dark chocolates. Varying the contribution of Sugars and altogether removing it from the projection is where the difference becomes apparent. When a frame with contribution primarily from Fiber is examined observation 22 looks more like a milk chocolate.

It would also be interesting to explore an inverse misclassification. In this case, a milk chocolate is selected while it was misclassified as a dark chocolate. Chocolate 84 is just this case and is compared with a correctly predicted milk chocolate (observation 71). The corresponding global view and radial tour frames are shown in Figure \ref{fig:casechocolatesinverse}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{./figures/case_chocolates_inverse} 

}

\caption{Examining the LVA for a PI which is milk (green) chocolate incorrectly predicted to be dark (orange). In the attribution projection, the PI could be either milk or dark. Sodium and Fiber have the largest differences in attributed variable importance, with low values relative to other milk chocolates. The lack of importance attributed to these variables is suspected of contributing to the mistake, so the contribution of Sodium is varied. If Sodium had a larger contribution to the prediction (like in this view). the PI would look more like other milk chocolates. (A video of the tour animation can be found at https://vimeo.com/666431148.)}\label{fig:casechocolatesinverse}
\end{figure}

The difference of position in the tree SHAP PCA with the previous case is quite significant; this gives a higher-level sense that the attributions should be quite different. Looking at the attribution projection, this is found to be the case. Previously, Fiber was essential while it is absent from the attribution in this case. Conversely, Calories from Fat and Total Fat have high attributions here, while they were unimportant in the preceding case.

Comparing the attribution with the CI (dotted line), large discrepancies in Sodium and Fiber are identified. The contribution of Sodium is selected to be varied. Even in the initial projection, the observation looks slightly more like its observed milk than predicted dark chocolate. The misclassification appears least supported when the basis reaches sodium attribution of typical dark chocolate.

\hypertarget{fifa-wage-regression}{%
\subsection{FIFA, Wage Regression}\label{fifa-wage-regression}}

The 2020 season FIFA data (Leone 2020; Biecek 2018) contains many skill measurements of soccer/football players and wage information. Nine higher-level skill groupings were identified and aggregated from highly correlated variables. A random forest model is fit from these predictors, regressing player wages {[}2020 euros{]}. The model was fit from 5000 observations before being thinned to 500 players to mitigate occlusion and render time. Continuing from the exploration in Section \textbackslash ref\{sec:explanations), we are interested to see the difference in attribution based on the exogenous player position. That is, the model should be able to use multiple linear profiles to better predict the wages from different field positions of players despite not having this information. A leading offensive fielder (L. Messi) is compared with a top defensive fielder (V. van Dijk). The same observations were used in Figure \ref{fig:shapdistrbd}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{./figures/case_fifa} 

}

\caption{Exploring the wages relative to skill measurements in the FIFA 2020 data. Star offensive player (L. Messi) is the PI, and he is compared with a top defensive player (V. van Dijk). The attribution projection is shown on the left, and it can be seen that this combination of variables produces a view where Messi has very high predicted (and observed) wages. Defense (`def`) is the chosen variable to vary. It starts very low, and Messi's predicted wages decrease dramatically as its contribution increases (right plot). The increased contribution in defense comes at the expense of offensive and reaction skills. The interpretation is that Messi's high wages are most attributable to his offensive and reaction skills, as initially provided by the LVA. (A video of the animated radial tour can be found at https://vimeo.com/666431163.)}\label{fig:casefifa}
\end{figure}

Figure \ref{fig:casefifa} tests the support of the LVA. Offensive and reaction skills (\texttt{off} and \texttt{rct}) are both crucial to explaining a star offensive player. If either of them were rotated out, the other would be rotated into the frame, maintaining a far-right position. However, increasing the contribution of a variable with low importance would rotate both variables out of the frame.

The contribution from \texttt{def} will be varied to contrast with offensive skills. As the contribution of defensive skills increases, Messi's is no longer separated from the group. Players with high values in defensive skills are now the rightmost points. In terms of what-if analysis, the difference between the data mean and his predicted wages would be halved if Messi's tree SHAP attributions were at these levels.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{./figures/case_ames2018} 

}

\caption{Exploring an observation with an extreme residual as the PI in relation to an observation with an accurate prediction for a similarly priced house in a random forest fit to the Ames housing data. The LVA indicates a sizable attribution to Lot Area (`LtA`), while the CI has minimal attribution to this variable. The PI has a higher predicted value than the CI in the attribution projection. Reducing the contribution of Lot Area brings these two prices in line. This suggests that if the model did not value Lot Area so highly for this observation, then the observed sales price would be quite similar. That is, the large residual is due to a lack of factoring in the Lot Area for the prediction of PI's sales price. (A video showing the animation is at https://vimeo.com/666431134.)}\label{fig:caseames}
\end{figure}

\hypertarget{ames-housing-2018-sales-price-regression}{%
\subsection{Ames Housing 2018, Sales Price Regression}\label{ames-housing-2018-sales-price-regression}}

Ames housing data 2018 (De Cock 2011; prevek18 2018) was subset to North Ames (the neighborhood with the most house sales). The remaining are 338 house sales. A random forest model was fit, predicting the sale price {[}USD{]} from the property variables: Lot Area (\texttt{LtA}), Overall Quality (\texttt{Qlt}), Year the house was Built (\texttt{YrB}), Living Area (\texttt{LvA}), number of Bathrooms (\texttt{Bth}), number of Bedrooms (\texttt{Bdr}), the total number of Rooms (\texttt{Rms}), Year the Garage was Built (\texttt{GYB}), and Garage Area (\texttt{GrA}). Using interactions with the global view, a house with an extreme negative residual and an accurate observation with a similar prediction is selected.

Figure \ref{fig:caseames} selects the house sale 74, a sizable under-prediction with an enormous Lot Area contribution. The CI has a similar predicted price though the prediction was accurate and gives almost no attribution to lot size. The attribution projection places observations with high Living Areas to the right. The contribution of Living Area contrasts the contribution of this variable. As the contribution of Lot Area decreases, the predictive power decreases for the PI, while the CI remains stationary. This large importance in the Living Area is relatively uncommon. Boosting tree models may be more resilient to such an under-prediction as they would up-weighting this residual and force its inclusion in the final model.

\hypertarget{sec:cheemdiscussion}{%
\section{Discussion}\label{sec:cheemdiscussion}}

There is a clear need to extend the interpretability of black box models. With techniques such as SHAP, LIME, Break-down, one can calculate LEs, i.e.~for every observation in the data. These techniques quantify for each observation how strongly particular variables affect the model's predictions. Surprisingly few techniques allow us to understand the global distribution of these LEs. Unsupervised data exploration techniques applied to data show how useful they are for identifying outliers, identifying clusters of observations or discovering correlations between variables. All of these tasks can be performed for a set of explanations.

To address this challenge this paper provides a technique that builds on LEs to explore the variable importance local to an observation. The LVA is converted into an attribution projection from which variable contributions are varied using a radial tour. Several diagnostic plots are provided to assist with understanding the sensitivity of the prediction to particular variables. A global view shows the data space, explanation space, and residual plot. The user can interactively select observations to compare, contrast, and study further. Then the radial tour is used to explore the variable sensitivity identified by the attribution projection.

This approach has been illustrated using four data examples of random forest models with the tree SHAP LVA. LEs focus on the model fit and help to dissect which variables are most responsible for the fitted value. They can also form the basis of learning how the model has got it wrong, when the observation is misclassified or has a large residual.

In the penguins example, we showed how the misclassification of a penguin arose due to it having an unusually small flipper size compared to others of its species. This was verified by making a follow-up plot of the data. The chocolates example shows how a dark chocolate was misclassified primarily due to its attribution to Fiber, and a milk chocolate was misclassified as dark due to its lowish Sodium value. In the FIFA example, we show how low Messi's salary would be if it depended on their defensive skill. In the Ames housing data, an inaccurate prediction for a house was likely due to the lot area not being effectively used by the random forest model.

This analysis is manually intensive and thus only feasible for investigating a few observations. The recommended approach is to investigate an observation where the model has not predicted accurately and compare it with an observation with similar predictor values where the model fitted well. The radial tour launches from the attribution projection to enable exploration of the sensitivity of the prediction to any variable. It can be helpful to make additional plots of the variables and responses to cross-check interpretations made from the cheem viewer. This methodology provides an additional tool in the box for studying model fitting.

\hypertarget{sec:infrastructure}{%
\section{Package Infrastructure}\label{sec:infrastructure}}

An implementation is provided in the open-source \textbf{R} package \textbf{cheem}, available on CRAN at \url{https://CRAN.R-project.org/package=cheem}. Example data sets are provided, and you can upload your data after model fitting and computing the LVAs. The LVAs need to be pre-computed and uploaded. Examples show how to do this for tree SHAP values, using \textbf{treeshap} (tree-based models from \textbf{gbm}, \textbf{lightgbm}, \textbf{randomForest}, \textbf{ranger}, or \textbf{xgboost} Greenwell et al. (2020); Shi et al. (2022); Liaw and Wiener (2002); Wright and Ziegler (2017); Chen et al. (2021), respectively). The SHAP and oscillation explanations could be easily added using \texttt{DALEX::explain()} (Biecek 2018; Biecek and Burzykowski 2021).

The application was made with \textbf{shiny} (Chang et al. 2021). The tour visual is built with \textbf{spinifex} (Spyrison and Cook 2020). Both views are created first with \textbf{ggplot2} (Wickham 2016) and then rendered as interactive \texttt{html} widgets with \textbf{plotly} (Sievert 2020). \textbf{DALEX} (Biecek 2018) and \emph{Explanatory Model Analysis} (Biecek and Burzykowski 2021) are helpful for understanding LEs and how to apply them.

The package can be installed from CRAN, and the application can be run using the following \textbf{R} code:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"cheem"}\NormalTok{, }\AttributeTok{dependencies =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{library}\NormalTok{(}\StringTok{"cheem"}\NormalTok{)}
\FunctionTok{run\_app}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Alternatively,

\begin{itemize}
\tightlist
\item
  A version of the cheem viewer shiny app can be directly accessed at
  \url{https://ebsmonash.shinyapps.io/cheem/}.
\item
  The development version of the package is available at \url{https://github.com/nspyrison/cheem}, and
\item
  Documentation of the package can be found at \url{https://nspyrison.github.io/cheem/}.
\end{itemize}

Follow the examples provided with the package to compute the LVAs (using \texttt{?cheem\_ls}). The application expects the output returned by \texttt{cheem\_ls()}, saved to an \texttt{rds} file with \texttt{saveRDS()} to be uploaded.

\hypertarget{acknowledgments}{%
\subsection*{Acknowledgments}\label{acknowledgments}}
\addcontentsline{toc}{subsection}{Acknowledgments}

Kim Marriott provided advice on many aspects of this work, especially on the explanations in the applications section. This research was supported by the Australian Government Research Training Program (RTP) scholarships. Thanks to Jieyang Chong for helping proofread this article. The namesake, Cheem, refers to a fictional race of humanoid trees from Doctor Who lore. \textbf{DALEX} pulls on from that universe, and we initially apply tree SHAP explanations specific to tree-based models.

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-adadi_peeking_2018}{}}%
Adadi, Amina, and Mohammed Berrada. 2018. {``Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence ({XAI}).''} \emph{IEEE Access} 6: 52138--60.

\leavevmode\vadjust pre{\hypertarget{ref-anderson_introduction_1995}{}}%
Anderson, James A. 1995. \emph{An Introduction to Neural Networks}. MIT press.

\leavevmode\vadjust pre{\hypertarget{ref-asimov_grand_1985}{}}%
Asimov, Daniel. 1985. {``The {Grand} {Tour}: A {Tool} for {Viewing} {Multidimensional} {Data}.''} \emph{SIAM Journal on Scientific and Statistical Computing} 6 (1): 128--43. \url{https://doi.org/10.1137/0906011}.

\leavevmode\vadjust pre{\hypertarget{ref-arrieta_explainable_2020}{}}%
Barredo Arrieta, Alejandro, Natalia Diaz-Rodriguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, et al. 2020. {``Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges Toward Responsible AI.''} \emph{Information Fusion} 58: 82--115. https://doi.org/\url{https://doi.org/10.1016/j.inffus.2019.12.012}.

\leavevmode\vadjust pre{\hypertarget{ref-biecek_dalex_2018}{}}%
Biecek, Przemyslaw. 2018. {``{DALEX}: Explainers for Complex Predictive Models in {R}.''} \emph{The Journal of Machine Learning Research} 19 (1): 3245--49.

\leavevmode\vadjust pre{\hypertarget{ref-biecek_ceterisparibus_2020}{}}%
---------. 2020. \emph{{ceterisParibus}: {Ceteris} {Paribus} {Profiles}}. \url{https://CRAN.R-project.org/package=ceterisParibus}.

\leavevmode\vadjust pre{\hypertarget{ref-biecek_explanatory_2021}{}}%
Biecek, Przemyslaw, and Tomasz Burzykowski. 2021. \emph{Explanatory {Model} {Analysis}: {Explore}, {Explain}, and {Examine} {Predictive} {Models}}. CRC Press.

\leavevmode\vadjust pre{\hypertarget{ref-boser_training_1992}{}}%
Boser, Bernhard E., Isabelle M. Guyon, and Vladimir N. Vapnik. 1992. {``A Training Algorithm for Optimal Margin Classifiers.''} In \emph{Proceedings of the Fifth Annual Workshop on {Computational} Learning Theory}, 144--52.

\leavevmode\vadjust pre{\hypertarget{ref-breiman_random_2001}{}}%
Breiman, Leo. 2001a. {``Random Forests.''} \emph{Machine Learning} 45 (1): 5--32.

\leavevmode\vadjust pre{\hypertarget{ref-breiman_statistical_2001}{}}%
---------. 2001b. {``Statistical Modeling: {The} Two Cultures (with Comments and a Rejoinder by the Author).''} \emph{Statistical Science} 16 (3): 199--231.

\leavevmode\vadjust pre{\hypertarget{ref-buja_grand_1986}{}}%
Buja, Andreas, and Daniel Asimov. 1986. {``Grand {Tour} {Methods}: {An} {Outline}.''} In \emph{Proceedings of the {Seventeenth} {Symposium} on the {Interface} of {Computer} {Sciences} and {Statistics} on {Computer} {Science} and {Statistics}}, 63--67. New York, NY, USA: Elsevier North-Holland, Inc. \url{http://dl.acm.org/citation.cfm?id=26036.26046}.

\leavevmode\vadjust pre{\hypertarget{ref-Caragea2008}{}}%
Caragea, Doina, Dianne Cook, Hadley Wickham, and Vasant Honavar. 2008. {``Visual Methods for Examining SVM Classifiers.''} In \emph{Visual Data Mining: Theory, Techniques and Tools for Visual Analytics}, edited by Simeon J. Simoff, Michael H. Böhlen, and Arturas Mazeika, 136--53. Berlin, Heidelberg: Springer Berlin Heidelberg. \url{https://doi.org/10.1007/978-3-540-71080-6_10}.

\leavevmode\vadjust pre{\hypertarget{ref-chang_shiny_2021}{}}%
Chang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke, Yihui Xie, Jeff Allen, Jonathan McPherson, Alan Dipert, and Barbara Borges. 2021. \emph{Shiny: Web Application Framework for r}. \url{https://CRAN.R-project.org/package=shiny}.

\leavevmode\vadjust pre{\hypertarget{ref-chen_xgboost_2021}{}}%
Chen, Tianqi, Tong He, Michael Benesty, Vadim Khotilovich, Yuan Tang, Hyunsu Cho, Kailong Chen, et al. 2021. {``Xgboost: {Extreme} {Gradient} {Boosting}.''} \url{https://CRAN.R-project.org/package=xgboost}.

\leavevmode\vadjust pre{\hypertarget{ref-cook_manual_1997}{}}%
Cook, Dianne, and Andreas Buja. 1997. {``Manual {Controls} for {High}-{Dimensional} {Data} {Projections}.''} \emph{Journal of Computational and Graphical Statistics} 6 (4): 464--80. \url{https://doi.org/10.2307/1390747}.

\leavevmode\vadjust pre{\hypertarget{ref-cook_grand_2008}{}}%
Cook, Dianne, Andreas Buja, Eun-Kyung Lee, and Hadley Wickham. 2008. {``Grand {Tours}, {Projection} {Pursuit} {Guided} {Tours}, and {Manual} {Controls}.''} In \emph{Handbook of {Data} {Visualization}}, 295--314. Berlin, Heidelberg: Springer Berlin Heidelberg. \url{https://doi.org/10.1007/978-3-540-33037-0_13}.

\leavevmode\vadjust pre{\hypertarget{ref-cook_interactive_2007}{}}%
Cook, Dianne, Deborah F. Swayne, and A. Buja. 2007. \emph{Interactive and {Dynamic} {Graphics} for {Data} {Analysis}: {With} {R} and {GGobi}}. Springer Science \& Business Media.

\leavevmode\vadjust pre{\hypertarget{ref-da_silva_projection_2021}{}}%
da Silva, Natalia, Dianne Cook, and Eun-Kyung Lee. 2021. {``A {Projection} {Pursuit} {Forest} {Algorithm} for {Supervised} {Classification}.''} \emph{Journal of Computational and Graphical Statistics}, 1--21.

\leavevmode\vadjust pre{\hypertarget{ref-de_cock_ames_2011}{}}%
De Cock, Dean. 2011. {``Ames, {Iowa}: {Alternative} to the {Boston} Housing Data as an End of Semester Regression Project.''} \emph{Journal of Statistics Education} 19 (3).

\leavevmode\vadjust pre{\hypertarget{ref-gorman_ecological_2014}{}}%
Gorman, Kristen B., Tony D. Williams, and William R. Fraser. 2014. {``Ecological Sexual Dimorphism and Environmental Variability Within a Community of {Antarctic} Penguins (Genus {Pygoscelis}).''} \emph{PloS One} 9 (3): e90081.

\leavevmode\vadjust pre{\hypertarget{ref-gosiewska_ibreakdown_2019}{}}%
Gosiewska, Alicja, and Przemyslaw Biecek. 2019. {``{IBreakDown}: {Uncertainty} of Model Explanations for Non-Additive Predictive Models.''} \emph{arXiv Preprint arXiv:1903.11420}.

\leavevmode\vadjust pre{\hypertarget{ref-greenwell_fastshap_2020}{}}%
Greenwell, Brandon. 2020. \emph{Fastshap: {Fast} {Approximate} {Shapley} {Values}}. \url{https://CRAN.R-project.org/package=fastshap}.

\leavevmode\vadjust pre{\hypertarget{ref-fastshap}{}}%
---------. 2021. \emph{Fastshap: Fast Approximate Shapley Values}. \url{https://CRAN.R-project.org/package=fastshap}.

\leavevmode\vadjust pre{\hypertarget{ref-greenwell_gbm_2020}{}}%
Greenwell, Brandon, Bradley Boehmke, Jay Cunningham, and G. B. M. Developers. 2020. {``Gbm: {Generalized} {Boosted} {Regression} {Models}.''} \url{https://CRAN.R-project.org/package=gbm}.

\leavevmode\vadjust pre{\hypertarget{ref-horst_palmerpenguins_2020}{}}%
Horst, Allison Marie, Alison Presmanes Hill, and Kristen B. Gorman. 2020. {``Palmerpenguins: {Palmer} {Archipelago} ({Antarctica}) Penguin Data.''} \url{https://allisonhorst.github.io/palmerpenguins/}.

\leavevmode\vadjust pre{\hypertarget{ref-karwowski_international_2006}{}}%
Karwowski, Waldemar. 2006. \emph{International {Encyclopedia} of {Ergonomics} and {Human} {Factors}, -3 {Volume} {Set}}. CRC Press.

\leavevmode\vadjust pre{\hypertarget{ref-kominsarczyk_treeshap_2021}{}}%
Kominsarczyk, Konrad, Pawel Kozminski, Szymon Maksymiuk, and Przemyslaw Biecek. 2021. {``Treeshap.''} Model Oriented. \url{https://github.com/ModelOriented/treeshap}.

\leavevmode\vadjust pre{\hypertarget{ref-PPtreeregViz}{}}%
Lee, Eun-Kyung, and HyunSun Cho. 2022. \emph{PPtreeregViz: Projection Pursuit Regression Tree Visualization}. \url{https://CRAN.R-project.org/package=PPtreeregViz}.

\leavevmode\vadjust pre{\hypertarget{ref-lee_state_2021}{}}%
Lee, Stuart, Dianne Cook, Natalia da Silva, Ursula Laa, Nicholas Spyrison, Earo Wang, and H. Sherry Zhang. 2021. {``The State-of-the-Art on Tours for Dynamic Visualization of High-Dimensional Data.''} \emph{WIREs Computational Statistics} n/a (n/a): e1573. \url{https://doi.org/10.1002/wics.1573}.

\leavevmode\vadjust pre{\hypertarget{ref-lee_pptree_2013}{}}%
Lee, Yoon Dong, Dianne Cook, Ji-won Park, and Eun-Kyung Lee. 2013. {``{PPtree}: {Projection} Pursuit Classification Tree.''} \emph{Electronic Journal of Statistics} 7: 1369--86.

\leavevmode\vadjust pre{\hypertarget{ref-leone_fifa_2020}{}}%
Leone, Stefano. 2020. {``{FIFA} 20 Complete Player Dataset.''} \url{https://kaggle.com/stefanoleone992/fifa-20-complete-player-dataset}.

\leavevmode\vadjust pre{\hypertarget{ref-liaw_classification_2002}{}}%
Liaw, Andy, and Matthew Wiener. 2002. {``Classification and Regression by {randomForest}.''} \emph{R News} 2 (3): 18--22.

\leavevmode\vadjust pre{\hypertarget{ref-lundberg_consistent_2018}{}}%
Lundberg, Scott M., Gabriel G. Erion, and Su-In Lee. 2018. {``Consistent Individualized Feature Attribution for Tree Ensembles.''} \emph{arXiv Preprint arXiv:1802.03888}.

\leavevmode\vadjust pre{\hypertarget{ref-lundberg_unified_2017}{}}%
Lundberg, Scott M., and Su-In Lee. 2017. {``A Unified Approach to Interpreting Model Predictions.''} In \emph{Proceedings of the 31st International Conference on Neural Information Processing Systems}, 4768--77.

\leavevmode\vadjust pre{\hypertarget{ref-flashlight}{}}%
Mayer, Michael. 2023a. \emph{Flashlight: Shed Light on Black Box Machine Learning Models}. \url{https://CRAN.R-project.org/package=flashlight}.

\leavevmode\vadjust pre{\hypertarget{ref-shapviz}{}}%
---------. 2023b. \emph{Shapviz: SHAP Visualizations}. \url{https://CRAN.R-project.org/package=shapviz}.

\leavevmode\vadjust pre{\hypertarget{ref-kernelshap}{}}%
Mayer, Michael, and David Watson. 2023. \emph{Kernelshap: Kernel SHAP}. \url{https://CRAN.R-project.org/package=kernelshap}.

\leavevmode\vadjust pre{\hypertarget{ref-molnar2022}{}}%
Molnar, Christoph. 2022. \emph{Interpretable Machine Learning: A Guide for Making Black Box Models Explainable}. 2nd ed. \url{https://christophm.github.io/interpretable-ml-book}.

\leavevmode\vadjust pre{\hypertarget{ref-ocagne_coordonnees_1885}{}}%
Ocagne, Maurice d'. 1885. \emph{Coordonnées Parallèles Et Axiales. {Méthode} de Transformation Géométrique Et Procédé Nouveau de Calcul Graphique Déduits de La Considération Des Coordonnées Parallèles, Par {Maurice} d'{Ocagne}, ...} Paris: Gauthier-Villars.

\leavevmode\vadjust pre{\hypertarget{ref-prevek18_ames_2018}{}}%
prevek18. 2018. {``Ames {Housing} {Dataset}.''} \emph{Kaggle.com}. \url{https://kaggle.com/prevek18/ames-housing-dataset}.

\leavevmode\vadjust pre{\hypertarget{ref-ribeiro_why_2016}{}}%
Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. {``"{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}.''} In \emph{Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}}, 1135--44. {KDD} '16. New York, NY, USA: Association for Computing Machinery. \url{https://doi.org/10.1145/2939672.2939778}.

\leavevmode\vadjust pre{\hypertarget{ref-roberts_state_2007}{}}%
Roberts, Jonathan C. 2007. {``State of the Art: {Coordinated} \& Multiple Views in Exploratory Visualization.''} In \emph{Fifth International Conference on Coordinated and Multiple Views in Exploratory Visualization ({CMV} 2007)}, 61--71. IEEE.

\leavevmode\vadjust pre{\hypertarget{ref-ExplainPrediction}{}}%
Robnik-Sikonja, Marko. 2018. \emph{ExplainPrediction: Explanation of Predictions for Classification and Regression Models}. \url{https://CRAN.R-project.org/package=ExplainPrediction}.

\leavevmode\vadjust pre{\hypertarget{ref-shapr}{}}%
Sellereite, Nikolai, Martin Jullum, and Annabelle Redelmeier. 2023. \emph{Shapr: Prediction Explanation with Dependence-Aware Shapley Values}. \url{https://CRAN.R-project.org/package=shapr}.

\leavevmode\vadjust pre{\hypertarget{ref-shapley_value_1953}{}}%
Shapley, Lloyd S. 1953. \emph{A Value for n-Person Games}. Princeton University Press.

\leavevmode\vadjust pre{\hypertarget{ref-shi_lightgbm_2022}{}}%
Shi, Yu, Guolin Ke, Damien Soukhavong, James Lamb, Qi Meng, Thomas Finley, Taifeng Wang, et al. 2022. {``Lightgbm: {Light} {Gradient} {Boosting} {Machine}.''} \url{https://CRAN.R-project.org/package=lightgbm}.

\leavevmode\vadjust pre{\hypertarget{ref-shmueli_explain_2010}{}}%
Shmueli, Galit. 2010. {``To Explain or to Predict?''} \emph{Statistical Science} 25 (3): 289--310.

\leavevmode\vadjust pre{\hypertarget{ref-shrikumar_learning_2017}{}}%
Shrikumar, Avanti, Peyton Greenside, and Anshul Kundaje. 2017. {``Learning Important Features Through Propagating Activation Differences.''} In \emph{International {Conference} on {Machine} {Learning}}, 3145--53. PMLR.

\leavevmode\vadjust pre{\hypertarget{ref-shrikumar_not_2016}{}}%
Shrikumar, Avanti, Peyton Greenside, Anna Shcherbina, and Anshul Kundaje. 2016. {``Not Just a Black Box: {Learning} Important Features Through Propagating Activation Differences.''} \emph{arXiv Preprint arXiv:1605.01713}.

\leavevmode\vadjust pre{\hypertarget{ref-sievert_interactive_2020}{}}%
Sievert, Carson. 2020. \emph{Interactive {Web}-{Based} {Data} {Visualization} with {R}, Plotly, and Shiny}. Chapman; Hall/CRC. \url{https://plotly-r.com}.

\leavevmode\vadjust pre{\hypertarget{ref-simonyan_deep_2014}{}}%
Simonyan, Karen, Andrea Vedaldi, and Andrew Zisserman. 2014. {``Deep Inside Convolutional Networks: {Visualising} Image Classification Models and Saliency Maps.''} In \emph{In {Workshop} at {International} {Conference} on {Learning} {Representations}}. Citeseer.

\leavevmode\vadjust pre{\hypertarget{ref-spyrison_spinifex_2020}{}}%
Spyrison, Nicholas, and Dianne Cook. 2020. {``Spinifex: An {R} {Package} for {Creating} a {Manual} {Tour} of {Low}-Dimensional {Projections} of {Multivariate} {Data}.''} \emph{The R Journal} 12 (1): 243. \url{https://doi.org/10.32614/RJ-2020-027}.

\leavevmode\vadjust pre{\hypertarget{ref-stahl-ethics}{}}%
Stahl, Bernd Carsten. 2021. {``Ethical Issues of AI.''} \emph{Artificial Intelligence for a Better Future}, 35--53. \url{https://doi.org/10.1007/978-3-030-69978-9_4}.

\leavevmode\vadjust pre{\hypertarget{ref-strumbelj_efficient_2010}{}}%
Strumbelj, Erik, and Igor Kononenko. 2010. {``An Efficient Explanation of Individual Classifications Using Game Theory.''} \emph{The Journal of Machine Learning Research} 11: 1--18.

\leavevmode\vadjust pre{\hypertarget{ref-unwin_ensemble_2018}{}}%
Unwin, Antony, and Pedro Valero-Mora. 2018. {``Ensemble {Graphics}.''} \emph{Journal of Computational and Graphical Statistics} 27 (1): 157--65. \url{https://doi.org/10.1080/10618600.2017.1383264}.

\leavevmode\vadjust pre{\hypertarget{ref-vanni_textual_2018}{}}%
Vanni, Laurent, Mélanie Ducoffe, Carlos Aguilar, Frédéric Precioso, and Damon Mayaffre. 2018. {``Textual {Deconvolution} {Saliency} ({TDS}): A Deep Tool Box for Linguistic Analysis.''} In \emph{Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})}, 548--57.

\leavevmode\vadjust pre{\hypertarget{ref-wickham_ggplot2_2016}{}}%
Wickham, Hadley. 2016. \emph{Ggplot2: {Elegant} {Graphics} for {Data} {Analysis}}. Springer-Verlag New York. \url{https://ggplot2.tidyverse.org}.

\leavevmode\vadjust pre{\hypertarget{ref-wickham_visualizing_2015}{}}%
Wickham, Hadley, Dianne Cook, and Heike Hofmann. 2015. {``Visualizing Statistical Models: {Removing} the Blindfold.''} \emph{Statistical Analysis and Data Mining: The ASA Data Science Journal} 8 (4): 203--25. \url{https://doi.org/10.1002/sam.11271}.

\leavevmode\vadjust pre{\hypertarget{ref-wright_ranger_2017}{}}%
Wright, Marvin N., and Andreas Ziegler. 2017. {``Ranger: {A} {Fast} {Implementation} of {Random} {Forests} for {High} {Dimensional} {Data} in {C}++ and {R}.''} \emph{Journal of Statistical Software} 77 (1): 1--17. \url{https://doi.org/10.18637/jss.v077.i01}.

\end{CSLReferences}

\end{document}
